{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6kjBvaHOlr5s9W4G+Jmwg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayak-coding/Extract-Website-Email-and-Social-Accounts-From-Company-Name/blob/main/Extract_Official_Website%2C_All_Email_Addresses_%26_All_Social_Accounts_From_Company_Name.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import Libraries"
      ],
      "metadata": {
        "id": "wG6C8uyyLIFK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRz9JLHGKkce"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from googlesearch import search\n",
        "from google.colab import files\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extract Official Website From Company Name"
      ],
      "metadata": {
        "id": "LhMiXvCOLDnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(\"emb.xlsx\")\n",
        "# df = df.iloc[6001:7120,0:1]\n",
        "df"
      ],
      "metadata": {
        "id": "SZnDDlfqK5NG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_company_website(company_name):\n",
        "    query = f\"{company_name} official website\"\n",
        "    try:\n",
        "        result = next(search(query, num = 1, stop = 1))\n",
        "        #result = next(search(query, num_results=1))\n",
        "        return result\n",
        "    except StopIteration:\n",
        "        return None\n",
        "\n",
        "# Read the CSV file\n",
        "# df = pd.read_csv('Countrywise Fashion Company Details - Sheet12.csv')  #  the actual path to your CSV file\n",
        "df = df\n",
        "# Extract websites\n",
        "websites = []\n",
        "\n",
        "for company_name in df[\"Company_name\"]:\n",
        "    website = get_company_website(company_name)\n",
        "    websites.append(website)\n",
        "\n",
        "# Add the websites as a new column\n",
        "df['Website'] = websites\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "df.to_csv('updated_file.csv', index=False)  # desired output filename"
      ],
      "metadata": {
        "id": "a7a_oe3ILAFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extract Emails From Websites"
      ],
      "metadata": {
        "id": "0HZwJstZLPYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to extract email addresses from a webpage\n",
        "def extract_emails_from_url(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:  # Check if the request was successful\n",
        "        content = response.text\n",
        "        emails = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b', content)\n",
        "        return emails\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "# List of websites\n",
        "websites = [\n",
        "    'https://india.highcommission.gov.au/',\n",
        "'https://mumbai.consulate.gov.au/'\n",
        "]\n",
        "\n",
        "# Initialize lists to store website and email data\n",
        "website_list = []\n",
        "email_list = []\n",
        "\n",
        "# Iterate through the list of websites and extract emails\n",
        "for website in websites:\n",
        "    emails = extract_emails_from_url(website)\n",
        "    if emails:  # If emails are found\n",
        "        for email in emails:\n",
        "            website_list.append(website)\n",
        "            email_list.append(email)\n",
        "\n",
        "# Create a DataFrame\n",
        "data = {'Website': website_list, 'Email': email_list}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save DataFrame to Excel\n",
        "df.to_excel('emails.xlsx', index=False)\n",
        "print(\"Data saved to 'emails.xlsx' file.\")"
      ],
      "metadata": {
        "id": "mc94tYmjLZ60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#For Merging Website And Email Address Files"
      ],
      "metadata": {
        "id": "u159QYfgLfP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the first Excel file\n",
        "df1 = pd.read_csv(\"website5.csv\")\n",
        "\n",
        "# Read the second Excel file\n",
        "df2 = pd.read_csv(\"email_results (1).csv\")\n",
        "\n",
        "# Merge the two DataFrames based on the \"Website\" column\n",
        "merged_df = pd.merge(df1, df2, on=\"Website\", how=\"outer\")\n",
        "\n",
        "# Save the merged DataFrame to a new Excel file\n",
        "merged_df.to_excel(\"merged_file.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "p-qniqckLhO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extracting Both Official Website And All Email Adreesses Together"
      ],
      "metadata": {
        "id": "l6UkoPPVLn6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_company_website(company_name):\n",
        "    query = f\"{company_name} official website\"\n",
        "    try:\n",
        "        result = next(search(query, num=1, stop=1))\n",
        "        return result\n",
        "    except StopIteration:\n",
        "        return None\n",
        "\n",
        "def extract_emails_from_url(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            content = response.text\n",
        "            emails = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b', content)\n",
        "            return emails\n",
        "        else:\n",
        "            return []\n",
        "    except requests.exceptions.SSLError:\n",
        "        return []\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_excel('emb.xlsx')  # Adjust the path to your CSV file\n",
        "\n",
        "# Initialize lists to store website and email data\n",
        "website_list = []\n",
        "email_list = []\n",
        "\n",
        "# Iterate through the list of companies and extract websites and emails\n",
        "for company_name in df[\"Company_name\"]:\n",
        "    website = get_company_website(company_name)\n",
        "    if website:\n",
        "        emails = extract_emails_from_url(website)\n",
        "        website_list.append(website)\n",
        "        if emails:\n",
        "            email_list.append(\", \".join(emails))\n",
        "        else:\n",
        "            email_list.append(None)\n",
        "    else:\n",
        "        website_list.append(None)\n",
        "        email_list.append(None)\n",
        "\n",
        "# Add the website and email data as new columns\n",
        "df['Website'] = website_list\n",
        "df['Email'] = email_list\n",
        "\n",
        "# Save the updated DataFrame to an Excel file\n",
        "df.to_excel('company_data_with_emails.xlsx', index=False)\n",
        "print(\"Data saved to 'company_data_with_emails.xlsx' file.\")"
      ],
      "metadata": {
        "id": "8Js-aX9SLqjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting Social accounts (Linkedin, Facebook, Twitter, Instagram) From Official Website"
      ],
      "metadata": {
        "id": "8u4N6KHcSl85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_social_media_links(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    social_media_links = {}\n",
        "\n",
        "    # Define patterns to match social media URLs\n",
        "    social_media_patterns = {\n",
        "        'Facebook': r'https?://(?:www\\.)?facebook\\.com/[\\w.-]+',\n",
        "        'Twitter': r'https?://(?:www\\.)?twitter\\.com/[\\w.-]+',\n",
        "        'Instagram': r'https?://(?:www\\.)?instagram\\.com/[\\w.-]+',\n",
        "        'Linkedin': r'https?://(?:www\\.)?linkedin\\.com/company/[\\w.-]+'\n",
        "        # Add more patterns for other social media platforms if needed\n",
        "    }\n",
        "\n",
        "    for platform, pattern in social_media_patterns.items():\n",
        "        matches = soup.find_all('a', href=re.compile(pattern, re.IGNORECASE))\n",
        "        if matches:\n",
        "            # Join the links into a single string\n",
        "            social_media_links[platform] = ', '.join(link['href'] for link in matches)\n",
        "\n",
        "    return social_media_links\n",
        "\n",
        "# List of URLs\n",
        "urls = [\n",
        "    \"https://centurymedia360.com/\",\n",
        "    \"https://kpohealth.com/\",\n",
        "    # Add more URLs here\n",
        "]\n",
        "\n",
        "# Initialize an empty list to store data\n",
        "data = []\n",
        "\n",
        "for url in urls:\n",
        "    social_media_links = fetch_social_media_links(url)\n",
        "    if social_media_links:\n",
        "        # Add website URL to the dictionary\n",
        "        social_media_links['Website'] = url\n",
        "        data.append(social_media_links)\n",
        "\n",
        "# Create DataFrame from the data list\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Reorder columns as per your requirement\n",
        "df = df[['Website', 'Facebook', 'Twitter', 'Instagram', 'Linkedin']]\n",
        "\n",
        "# Save DataFrame to Excel file\n",
        "df.to_excel('social_media_accounts.xlsx', index=False)\n",
        "\n",
        "print(\"Social media account data saved to social_media_accounts.xlsx successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iU1_03M570Kz",
        "outputId": "08a584df-b88a-4352-8999-d8d652dc0295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Social media account data saved to social_media_accounts.xlsx successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting All Social Media Links And Emails From The Website Name"
      ],
      "metadata": {
        "id": "yUX2LyrTSyzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_company_website(company_name):\n",
        "    query = f\"{company_name} official website\"\n",
        "    try:\n",
        "        result = next(search(query, num=1, stop=1))\n",
        "        return result\n",
        "    except StopIteration:\n",
        "        return None\n",
        "\n",
        "def extract_emails_from_url(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            content = response.text\n",
        "            emails = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b', content)\n",
        "            return emails\n",
        "        else:\n",
        "            return []\n",
        "    except requests.exceptions.SSLError:\n",
        "        return []\n",
        "\n",
        "def fetch_social_media_links(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            response.raise_for_status()\n",
        "        else:\n",
        "            print(f\"Ignoring {url} due to status code: {response.status_code}\")\n",
        "            return None\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    social_media_links = {}\n",
        "\n",
        "    # Define patterns to match social media URLs\n",
        "    social_media_patterns = {\n",
        "        'Facebook': r'https?://(?:www\\.)?facebook\\.com/[\\w.-]+',\n",
        "        'Twitter': r'https?://(?:www\\.)?twitter\\.com/[\\w.-]+',\n",
        "        'Instagram': r'https?://(?:www\\.)?instagram\\.com/[\\w.-]+',\n",
        "        'Linkedin': r'https?://(?:www\\.)?linkedin\\.com/company/[\\w.-]+'\n",
        "        # Add more patterns for other social media platforms if needed\n",
        "    }\n",
        "\n",
        "    for platform, pattern in social_media_patterns.items():\n",
        "        matches = soup.find_all('a', href=re.compile(pattern, re.IGNORECASE))\n",
        "        if matches:\n",
        "            # Join the links into a single string\n",
        "            social_media_links[platform] = ', '.join(link['href'] for link in matches)\n",
        "\n",
        "    return social_media_links\n",
        "\n",
        "# Read the Excel file with website names\n",
        "df = pd.read_excel('company_test.xlsx')  # Adjust the path to your Excel file\n",
        "\n",
        "# Initialize lists to store website, email, and social media data\n",
        "website_list = []\n",
        "email_list = []\n",
        "social_media_data = []\n",
        "\n",
        "# Iterate through the list of companies and extract websites, emails, and social media accounts\n",
        "for company_name in df[\"Company_name\"]:\n",
        "    website = get_company_website(company_name)\n",
        "    if website:\n",
        "        emails = extract_emails_from_url(website)\n",
        "        social_media_links = fetch_social_media_links(website)\n",
        "\n",
        "        website_list.append(website)\n",
        "\n",
        "        if emails:\n",
        "            email_list.append(\", \".join(emails))\n",
        "        else:\n",
        "            email_list.append(None)\n",
        "\n",
        "        if social_media_links:\n",
        "            social_media_data.append(social_media_links)\n",
        "        else:\n",
        "            social_media_data.append(None)\n",
        "    else:\n",
        "        website_list.append(None)\n",
        "        email_list.append(None)\n",
        "        social_media_data.append(None)\n",
        "\n",
        "# Create DataFrame from the data lists\n",
        "df['Website'] = website_list\n",
        "df['Email'] = email_list\n",
        "\n",
        "# Extract social media links into separate columns\n",
        "for platform in ['Facebook', 'Twitter', 'Instagram', 'Linkedin']:\n",
        "    df[platform] = [data[platform] if data and platform in data else 'Not found' for data in social_media_data]\n",
        "\n",
        "# Reorder columns as per your requirement\n",
        "df = df[['Company_name', 'Website', 'Email', 'Facebook', 'Twitter', 'Instagram', 'Linkedin']]\n",
        "\n",
        "# Save the updated DataFrame to an Excel file\n",
        "df.to_excel('company_data_with_emails_and_social_media.xlsx', index=False)\n",
        "print(\"Data saved to 'company_data_with_emails_and_social_media.xlsx' file.\")"
      ],
      "metadata": {
        "id": "CR4RoVk8HdbY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}